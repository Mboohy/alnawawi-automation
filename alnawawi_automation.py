# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PjUHz77pUt0jRoAr2S_3tFT17JeddS4v
"""

#ALL


#invoices

from google.colab import auth
import gspread
from google.auth import default
import requests
from urllib.parse import urlencode
import pandas as pd
import numpy as np
from datetime import datetime

# Authenticate with Google
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Configuration
BASE_URL = "https://api.alnawawifiqh.com"
LOGIN_URL = f"{BASE_URL}/api/login"
PAYMENTS_URL = f"{BASE_URL}/admin/payments"
GOOGLE_SHEET_ID = "1khvh80UO5ug2H3Ntv_xNXvhDx0lQTMaCOLT9QKYgrr0"

def authenticate():
    """Authenticate with Alnawawi Fiqh API"""
    try:
        response = requests.post(
            LOGIN_URL,
            json={
                "email": "admin@alnawawifiqh.com",
                "password": "`P:;\\3%v>x!:T8&%"
            },
            timeout=10
        )
        response.raise_for_status()
        return response.json().get('token')
    except Exception as e:
        print(f"Authentication failed: {str(e)}")
        return None

def get_all_payments_data(token):
    """Fetch complete payments data with pagination"""
    today = datetime.now().strftime('%Y-%m-%d')
    all_data = []
    page = 1
    per_page = 500  # Maximum records per request
    has_more = True

    print("Fetching all payment records...")

    while has_more:
        params = {
            "format": "json",
            "page": page,
            "per_page": per_page,
            "from": "2025-07-10",  # Get all records
            "to": today,
            "type": "invoices",
            "status": "accepted"
        }

        try:
            response = requests.get(
                PAYMENTS_URL,
                headers={"Authorization": f"Bearer {token}"},
                params=params,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            # Debug: Print response structure
            print(f"Page {page} response keys:", data.keys() if isinstance(data, dict) else "List response")

            # Handle different response formats
            if isinstance(data, dict):
                if 'data' in data:  # Nested data
                    records = data['data']
                    # Check for pagination metadata
                    if 'last_page' in data and page >= data['last_page']:
                        has_more = False
                else:  # Data is the root object
                    records = data
                    has_more = False
            else:  # Direct list response
                records = data
                has_more = False if len(records) < per_page else True

            if not records:
                print("No records found in response")
                break

            all_data.extend(records)
            print(f"Fetched page {page} with {len(records)} records (Total: {len(all_data)})")

            page += 1

        except requests.exceptions.HTTPError as http_err:
            print(f"HTTP error occurred: {http_err}")
            break
        except Exception as e:
            print(f"Error fetching page {page}: {str(e)}")
            break

    if not all_data:
        print("No data received from API")
        return None

    print(f"\nTotal records fetched: {len(all_data)}")
    if all_data:
        print("Sample record fields:", list(all_data[0].keys()))
    return all_data

def upload_to_google_sheets(data):
    """Upload complete data to 'cash' worksheet with batch processing"""
    try:
        # Convert to DataFrame
        df = pd.DataFrame(data)

        # Clean data
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna('')

        print(f"\nColumns to upload ({len(df.columns)}): {df.columns.tolist()}")
        print(f"Total records to upload: {len(df)}")

        # Open the spreadsheet and get or create the 'cash' worksheet
        sheet = gc.open_by_key(GOOGLE_SHEET_ID)

        try:
            # Try to get the 'cash' worksheet
            worksheet = sheet.worksheet("cash")
            worksheet.clear()
            print("Found existing 'cash' worksheet, cleared contents")
        except gspread.exceptions.WorksheetNotFound:
            # If worksheet doesn't exist, create it
            worksheet = sheet.add_worksheet(title="cash", rows=1000, cols=20)
            print("Created new 'cash' worksheet")

        # Upload in optimized batches
        batch_size = 200  # Google Sheets API limit is 50000 cells per request
        total_rows = len(df)

        # Convert all values to strings to avoid serialization issues
        df = df.astype(str)

        # Calculate maximum safe batch size based on columns
        max_batch_rows = min(batch_size, 50000 // len(df.columns))

        # Upload headers and first batch
        first_batch = df.iloc[0:max_batch_rows]
        worksheet.update(
            [df.columns.values.tolist()] + first_batch.values.tolist(),
            value_input_option='USER_ENTERED'
        )
        print(f"Uploaded first {max_batch_rows} rows...")

        # Upload remaining batches
        for i in range(max_batch_rows, total_rows, max_batch_rows):
            batch = df.iloc[i:i+max_batch_rows]
            worksheet.append_rows(batch.values.tolist(), value_input_option='USER_ENTERED')
            print(f"Uploaded rows {i+1}-{min(i+max_batch_rows, total_rows)}...")

        print("✓ All data successfully uploaded to 'cash' worksheet")
        return True

    except Exception as e:
        print(f"Upload failed: {str(e)}")
        return False

# Main Execution
print("=== Alnawawi Fiqh Data Pipeline ===")
print(f"Current date: {datetime.now().strftime('%Y-%m-%d')}")

# 1. Authenticate
print("\n1. Authenticating with Alnawawi Fiqh API...")
if not (token := authenticate()):
    raise Exception("Authentication failed")
print("✓ Authentication successful")

# 2. Get Data
print("\n2. Fetching complete payments data...")
all_payments_data = get_all_payments_data(token)
if not all_payments_data:
    raise Exception("Failed to fetch data")

# 3. Upload Data
print("\n3. Uploading to 'cash' worksheet...")
if not upload_to_google_sheets(all_payments_data):
    raise Exception("Upload failed")

print("\n✅ All fields successfully updated in 'cash' worksheet")




#installments

from google.colab import auth
import gspread
from google.auth import default
import requests
from urllib.parse import urlencode
import pandas as pd
import numpy as np
from datetime import datetime

# Authenticate with Google
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Configuration
BASE_URL = "https://api.alnawawifiqh.com"
LOGIN_URL = f"{BASE_URL}/api/login"
PAYMENTS_URL = f"{BASE_URL}/admin/payments"
GOOGLE_SHEET_ID = "1khvh80UO5ug2H3Ntv_xNXvhDx0lQTMaCOLT9QKYgrr0"

def authenticate():
    """Authenticate with Alnawawi Fiqh API"""
    try:
        response = requests.post(
            LOGIN_URL,
            json={
                "email": "admin@alnawawifiqh.com",
                "password": "`P:;\\3%v>x!:T8&%"
            },
            timeout=10
        )
        response.raise_for_status()
        return response.json().get('token')
    except Exception as e:
        print(f"Authentication failed: {str(e)}")
        return None

def get_all_payments_data(token):
    """Fetch complete payments data with pagination"""
    today = datetime.now().strftime('%Y-%m-%d')
    all_data = []
    page = 1
    per_page = 500  # Maximum records per request
    has_more = True

    print("Fetching all payment records...")

    while has_more:
        params = {
            "format": "json",
            "page": page,
            "per_page": per_page,
            "from": "2025-07-10",  # Get all records
            "to": today,
            "type": "installments",
            "status": "accepted"
        }

        try:
            response = requests.get(
                PAYMENTS_URL,
                headers={"Authorization": f"Bearer {token}"},
                params=params,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            # Debug: Print response structure
            print(f"Page {page} response keys:", data.keys() if isinstance(data, dict) else "List response")

            # Handle different response formats
            if isinstance(data, dict):
                if 'data' in data:  # Nested data
                    records = data['data']
                    # Check for pagination metadata
                    if 'last_page' in data and page >= data['last_page']:
                        has_more = False
                else:  # Data is the root object
                    records = data
                    has_more = False
            else:  # Direct list response
                records = data
                has_more = False if len(records) < per_page else True

            if not records:
                print("No records found in response")
                break

            all_data.extend(records)
            print(f"Fetched page {page} with {len(records)} records (Total: {len(all_data)})")

            page += 1

        except requests.exceptions.HTTPError as http_err:
            print(f"HTTP error occurred: {http_err}")
            break
        except Exception as e:
            print(f"Error fetching page {page}: {str(e)}")
            break

    if not all_data:
        print("No data received from API")
        return None

    print(f"\nTotal records fetched: {len(all_data)}")
    if all_data:
        print("Sample record fields:", list(all_data[0].keys()))
    return all_data

def upload_to_google_sheets(data):
    """Upload complete data to 'installments' worksheet with batch processing"""
    try:
        # Convert to DataFrame
        df = pd.DataFrame(data)

        # Clean data
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna('')

        print(f"\nColumns to upload ({len(df.columns)}): {df.columns.tolist()}")
        print(f"Total records to upload: {len(df)}")

        # Open the spreadsheet and get the 'installments' worksheet
        sheet = gc.open_by_key(GOOGLE_SHEET_ID)

        try:
            # Try to get the 'installments' worksheet
            worksheet = sheet.worksheet("installments")
            worksheet.clear()
            print("Found existing 'installments' worksheet, cleared contents")
        except gspread.exceptions.WorksheetNotFound:
            # If worksheet doesn't exist, create it
            worksheet = sheet.add_worksheet(title="installments", rows=1000, cols=20)
            print("Created new 'installments' worksheet")

        # Upload in optimized batches
        batch_size = 200  # Google Sheets API limit is 50000 cells per request
        total_rows = len(df)

        # Convert all values to strings to avoid serialization issues
        df = df.astype(str)

        # Calculate maximum safe batch size based on columns
        max_batch_rows = min(batch_size, 50000 // len(df.columns))

        # Upload headers and first batch
        first_batch = df.iloc[0:max_batch_rows]
        worksheet.update(
            [df.columns.values.tolist()] + first_batch.values.tolist(),
            value_input_option='USER_ENTERED'
        )
        print(f"Uploaded first {max_batch_rows} rows...")

        # Upload remaining batches
        for i in range(max_batch_rows, total_rows, max_batch_rows):
            batch = df.iloc[i:i+max_batch_rows]
            worksheet.append_rows(batch.values.tolist(), value_input_option='USER_ENTERED')
            print(f"Uploaded rows {i+1}-{min(i+max_batch_rows, total_rows)}...")

        print("✓ All data successfully uploaded to 'installments' worksheet")
        return True

    except Exception as e:
        print(f"Upload failed: {str(e)}")
        return False

# Main Execution
print("=== Alnawawi Fiqh Data Pipeline ===")
print(f"Current date: {datetime.now().strftime('%Y-%m-%d')}")

# 1. Authenticate
print("\n1. Authenticating with Alnawawi Fiqh API...")
if not (token := authenticate()):
    raise Exception("Authentication failed")
print("✓ Authentication successful")

# 2. Get Data
print("\n2. Fetching complete payments data...")
all_payments_data = get_all_payments_data(token)
if not all_payments_data:
    raise Exception("Failed to fetch data")

# 3. Upload Data
print("\n3. Uploading to 'installments' worksheet...")
if not upload_to_google_sheets(all_payments_data):
    raise Exception("Upload failed")

print("\n✅ All fields successfully updated in 'installments' worksheet")





#students
from google.colab import auth
import gspread
from google.auth import default
import requests
import pandas as pd
import numpy as np
from datetime import datetime
import time

# Authenticate with Google
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Configuration
BASE_URL = "https://api.alnawawifiqh.com"
LOGIN_URL = f"{BASE_URL}/api/login"
STUDENTS_URL = f"{BASE_URL}/admin/accepted/users"
GOOGLE_SHEET_ID = "1khvh80UO5ug2H3Ntv_xNXvhDx0lQTMaCOLT9QKYgrr0"
MAX_RETRIES = 3
REQUEST_TIMEOUT = 40

def authenticate():
    """Authenticate with Alnawawi Fiqh API"""
    try:
        response = requests.post(
            LOGIN_URL,
            json={
                "email": "admin@alnawawifiqh.com",
                "password": "`P:;\\3%v>x!:T8&%"
            },
            timeout=20
        )
        response.raise_for_status()
        print("Authentication successful. Token received.")
        return response.json().get('token')
    except Exception as e:
        print(f"Authentication failed: {str(e)}")
        return None

def get_api_response(token, params):
    """Helper function to make API requests with retries"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(
                STUDENTS_URL,
                headers={"Authorization": f"Bearer {token}"},
                params=params,
                timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                wait_time = (2 ** attempt) * 5  # Exponential backoff
                print(f"Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
    return None

def get_all_students_data(token):
    """Fetch complete students data with enhanced error handling"""
    today = datetime.now().strftime('%Y-%m-%d')
    all_data = []
    page = 1
    per_page = 100

    print("\nTesting API connection with first page...")
    test_params = {
        "format": "json",
        "page": 1,
        "per_page": 1,
        "grade[]": 14,
        "year[]": 48,
        "from": "2024-06-01",
        "to": today,
        "field": "name"
    }

    # Test connection with small request
    test_response = get_api_response(token, test_params)
    if not test_response:
        print("Failed to get initial test response")
        return None

    try:
        test_data = test_response.json()
        print("Test response structure:", type(test_data))
        print("Test response keys:", test_data.keys())

        # Check if we got any actual student data
        if 'data' in test_data and isinstance(test_data['data'], list):
            if test_data['data']:
                sample_record = test_data['data'][0]
                print("\nSample student record fields:", list(sample_record.keys()))
            else:
                print("\nNo student records found in test response")
                print("Please check your filters - the response is empty")
                return None
        else:
            print("\nUnexpected response format")
            print("Full test response:", test_data)
            return None

    except ValueError as e:
        print("Failed to parse test response JSON:", str(e))
        print("Raw response:", test_response.text)
        return None

    print("\nStarting full data fetch...")

    while True:
        params = {
            "format": "json",
            "page": page,
            "per_page": per_page,
            "grade[]": 14,
            "year[]": 48,
            "from": "2024-06-01",
            "to": today,
            "field": "name"
        }

        response = get_api_response(token, params)
        if not response:
            print(f"Failed to get page {page} after {MAX_RETRIES} attempts")
            break

        try:
            data = response.json()

            if not isinstance(data, dict) or 'data' not in data:
                print(f"Unexpected response format in page {page}")
                break

            records = data['data']
            if not isinstance(records, list):
                print(f"Unexpected data format in page {page}")
                break

            if not records:
                print(f"No records found in page {page}")
                if page > 1:  # Stop if we get empty page after first page
                    break
                else:
                    print("First page is empty - stopping")
                    return None

            # Filter valid records
            valid_records = [r for r in records if isinstance(r, dict)]
            invalid_records = len(records) - len(valid_records)

            if invalid_records > 0:
                print(f"Filtered out {invalid_records} invalid records from page {page}")

            if valid_records:
                all_data.extend(valid_records)
                print(f"Page {page}: Added {len(valid_records)} valid records (Total: {len(all_data)})")

                # Check if we've reached the last page
                if 'last_page' in data and page >= data['last_page']:
                    break
            else:
                print(f"No valid records in page {page}")

            page += 1

            # Small delay between requests
            time.sleep(0.5)

        except ValueError as e:
            print(f"Failed to parse JSON from page {page}: {str(e)}")
            print("Raw response:", response.text)
            break
        except Exception as e:
            print(f"Unexpected error processing page {page}: {str(e)}")
            break

    if not all_data:
        print("\nNo valid student data received from API")
        return None

    print(f"\nSuccessfully fetched {len(all_data)} student records")
    return all_data

def upload_to_google_sheets(data, worksheet_name="students"):
    """Upload data to specified worksheet"""
    if not data:
        print("No data to upload")
        return False

    try:
        df = pd.DataFrame(data)
        df = df.replace([np.inf, -np.inf], np.nan).fillna('').astype(str)

        print(f"\nColumns to upload ({len(df.columns)}): {df.columns.tolist()}")
        print(f"Total records to upload: {len(df)}")

        sheet = gc.open_by_key(GOOGLE_SHEET_ID)

        try:
            worksheet = sheet.worksheet(worksheet_name)
            worksheet.clear()
            print(f"Cleared existing '{worksheet_name}' worksheet")
        except gspread.exceptions.WorksheetNotFound:
            worksheet = sheet.add_worksheet(
                title=worksheet_name,
                rows=max(1000, len(df)+100),
                cols=min(26, len(df.columns)+5))
            print(f"Created new '{worksheet_name}' worksheet")

        # Upload in chunks
        chunk_size = 200
        worksheet.update([df.columns.values.tolist()], 'A1')
        print("Uploaded headers")

        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            worksheet.append_rows(chunk.values.tolist(), value_input_option='USER_ENTERED')
            print(f"Uploaded rows {i+1}-{min(i+chunk_size, len(df))}")

        print(f"✓ Upload completed successfully to '{worksheet_name}'")
        return True

    except Exception as e:
        print(f"Upload failed: {str(e)}")
        return False

# Main Execution
print("=== Alnawawi Fiqh Students Data Pipeline ===")
print(f"Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# 1. Authenticate
print("\n1. Authenticating...")
token = authenticate()
if not token:
    raise Exception("Authentication failed")

# 2. Get Data
print("\n2. Fetching data...")
students_data = get_all_students_data(token)
if not students_data:
    print("\n⚠️ No student data found with current filters.")
    print("Please check:")
    print("- Your grade[] and year[] parameters")
    print("- The date range (from/to)")
    print("- Your API permissions")
    raise Exception("No student data available with current filters")

# 3. Upload Data
print("\n3. Uploading data...")
if not upload_to_google_sheets(students_data, "students"):
    raise Exception("Upload failed")

print("\n✅ Pipeline completed successfully")


#applicants
from google.colab import auth
import gspread
from google.auth import default
import requests
import pandas as pd
import numpy as np
from datetime import datetime
import json
import time

# Authenticate with Google
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Configuration
BASE_URL = "https://api.alnawawifiqh.com"
LOGIN_URL = f"{BASE_URL}/api/login"
APPLICANTS_URL = f"{BASE_URL}/admin/pending/users"
GOOGLE_SHEET_ID = "1khvh80UO5ug2H3Ntv_xNXvhDx0lQTMaCOLT9QKYgrr0"
MAX_RETRIES = 3
REQUEST_TIMEOUT = 40

def authenticate():
    """Authenticate with Alnawawi Fiqh API"""
    try:
        response = requests.post(
            LOGIN_URL,
            json={
                "email": "admin@alnawawifiqh.com",
                "password": "`P:;\\3%v>x!:T8&%"
            },
            timeout=20
        )
        response.raise_for_status()
        print("Authentication successful. Token received.")
        return response.json().get('token')
    except Exception as e:
        print(f"Authentication failed: {str(e)}")
        return None

def get_api_response(token, params):
    """Helper function to make API requests with retries"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(
                APPLICANTS_URL,
                headers={"Authorization": f"Bearer {token}"},
                params=params,
                timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                wait_time = (2 ** attempt) * 5  # Exponential backoff
                print(f"Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
    return None

def get_all_applicants_data(token):
    """Fetch complete applicants data with enhanced error handling"""
    today = datetime.now().strftime('%Y-%m-%d')
    all_data = []
    page = 1
    per_page = 100  # Reduced page size for testing

    print("\nTesting API connection with first page...")
    test_params = {
        "format": "json",
        "page": 1,
        "per_page": 1,
        "from": "2024-06-01",
        "to": today,
        "field": "name"
    }

    # Test connection with small request
    test_response = get_api_response(token, test_params)
    if not test_response:
        print("Failed to get initial test response")
        return None

    try:
        test_data = test_response.json()
        print("Test response structure:", type(test_data))

        if isinstance(test_data, dict):
            print("Test response keys:", test_data.keys())
            if 'data' in test_data:
                print("'data' field type:", type(test_data['data']))

                # Handle the actual data structure
                if isinstance(test_data['data'], dict):
                    if 'data' in test_data['data']:  # Nested data structure
                        records = test_data['data']['data']
                        if isinstance(records, list) and records:
                            print("\nSample applicant record fields:", list(records[0].keys()))
                            print("First record sample:", records[0])
                        else:
                            print("\nNo applicant records found in test response")
                            print("Full test response:", test_data)
                            return None
                    else:
                        print("\nUnexpected data structure in response")
                        print("Full test response:", test_data)
                        return None
        else:
            print("\nUnexpected response format")
            print("Full test response:", test_data)
            return None

    except ValueError as e:
        print("Failed to parse test response JSON:", str(e))
        print("Raw response:", test_response.text)
        return None

    print("\nStarting full data fetch...")

    while True:
        params = {
            "format": "json",
            "page": page,
            "per_page": per_page,
            "from": "2024-06-01",
            "to": today,
            "field": "name"
        }

        response = get_api_response(token, params)
        if not response:
            print(f"Failed to get page {page} after {MAX_RETRIES} attempts")
            break

        try:
            data = response.json()

            records = []
            if isinstance(data, dict) and 'data' in data:
                if isinstance(data['data'], dict) and 'data' in data['data']:
                    records = data['data']['data']
                else:
                    records = []

            if not records or not isinstance(records, list):
                print(f"No valid records found in page {page}")
                if page > 1:  # Stop if we get empty page after first page
                    break
                else:
                    print("First page is empty - stopping")
                    return None

            # Filter valid records
            valid_records = [r for r in records if isinstance(r, dict)]
            invalid_records = len(records) - len(valid_records)

            if invalid_records > 0:
                print(f"Filtered out {invalid_records} invalid records from page {page}")

            if valid_records:
                all_data.extend(valid_records)
                print(f"Page {page}: Added {len(valid_records)} valid records (Total: {len(all_data)})")
            else:
                print(f"No valid records in page {page}")

            page += 1

            # Small delay between requests
            time.sleep(0.5)

        except ValueError as e:
            print(f"Failed to parse JSON from page {page}: {str(e)}")
            print("Raw response:", response.text)
            break
        except Exception as e:
            print(f"Unexpected error processing page {page}: {str(e)}")
            break

    if not all_data:
        print("\nNo valid applicant data received from API")
        return None

    print(f"\nSuccessfully fetched {len(all_data)} applicant records")
    return all_data

def upload_to_google_sheets(data):
    """Upload data to 'applicants' worksheet"""
    if not data:
        print("No data to upload")
        return False

    try:
        df = pd.DataFrame(data)
        df = df.replace([np.inf, -np.inf], np.nan).fillna('').astype(str)

        print(f"\nColumns to upload ({len(df.columns)}): {df.columns.tolist()}")
        print(f"Total records to upload: {len(df)}")

        sheet = gc.open_by_key(GOOGLE_SHEET_ID)

        try:
            worksheet = sheet.worksheet("applicants")
            worksheet.clear()
            print("Cleared existing worksheet")
        except gspread.exceptions.WorksheetNotFound:
            worksheet = sheet.add_worksheet(
                title="applicants",
                rows=max(1000, len(df)+100),
                cols=min(26, len(df.columns)+5))
            print("Created new worksheet")

        # Upload in chunks
        chunk_size = 200
        worksheet.update([df.columns.values.tolist()], 'A1')
        print("Uploaded headers")

        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            worksheet.append_rows(chunk.values.tolist(), value_input_option='USER_ENTERED')
            print(f"Uploaded rows {i+1}-{min(i+chunk_size, len(df))}")

        print("✓ Upload completed successfully")
        return True

    except Exception as e:
        print(f"Upload failed: {str(e)}")
        return False

# Main Execution
print("=== Alnawawi Fiqh Applicants Data Pipeline ===")
print(f"Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# 1. Authenticate
print("\n1. Authenticating...")
token = authenticate()
if not token:
    raise Exception("Authentication failed")

# 2. Get Data
print("\n2. Fetching data...")
applicants_data = get_all_applicants_data(token)
if not applicants_data:
    raise Exception("Failed to fetch valid applicant data")

# 3. Upload Data
print("\n3. Uploading data...")
if not upload_to_google_sheets(applicants_data):
    raise Exception("Upload failed")

print("\n✅ Pipeline completed successfully")

